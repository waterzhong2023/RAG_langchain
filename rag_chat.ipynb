{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaK62oX3NhRf"
      },
      "source": [
        "# 基于langchain创建自己专属的对话大模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 领域精准问答\n",
        "2. 数据更新频繁\n",
        "3. 生成内容可解释可追溯\n",
        "4. 数据隐私保护"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-173FmLdNhRj"
      },
      "source": [
        "通过这个例子，我们将基于`LangChain`, `OpenAI(LLM)`,  `vector DB`构建一个属于自己的LLM模型。\n",
        "\n",
        "主要使用的技术————***Retrieval Augmented Generation (RAG)***\n",
        "\n",
        "首先确保自己拥有一个 `OpenAI API key` (也并非必须)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzzHZ2sLNhRk"
      },
      "source": [
        "### 准备环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAivifIqNhRk",
        "outputId": "53e3378c-79df-43e8-c92a-c0dce00dcaa0"
      },
      "outputs": [],
      "source": [
        "! pip install -qU \\\n",
        "    langchain==0.0.316 \\\n",
        "    openai==0.28.1  \\\n",
        "    tiktoken==0.5.1  \\\n",
        "    cohere \\\n",
        "    chromadb==0.4.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eg0ay3TNhRm"
      },
      "source": [
        "### 创建一个对话模型(no RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yd3OUQIWNhRm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxQlUl1Cwom"
      },
      "source": [
        "OpenAI Python 的例子\n",
        "```python\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Orange.\"},\n",
        "]\n",
        "```\n",
        "https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models\n",
        "\n",
        "\n",
        "但是langchain 需要使用以下的格式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jbPKtHYsNhRn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Knock knock.\"),\n",
        "    AIMessage(content=\"Who's there?\"),\n",
        "    HumanMessage(content=\"Orange\"),\n",
        "    \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11iDGkWPNhRn",
        "outputId": "e3698bd8-b14b-4abf-957f-041c0abd7f7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Orange who?')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = chat(messages)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7db28OOMjig"
      },
      "source": [
        "因为 `res`也是`AIMessage`属性，所以我们可以直接进行添加，即可实现下一次的响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85dW12laNLmO",
        "outputId": "7e431e35-7673-41d3-8ac7-a4b352c1b1be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orange you glad I'm here to assist you?\n"
          ]
        }
      ],
      "source": [
        "messages.append(res)\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YtYH-1oNhRo",
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "#### 处理LLM存在的缺陷\n",
        "1. 容易出现幻觉\n",
        "2. 信息滞后\n",
        "3. 专业领域深度知识匮乏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1PZxdF06NhRp"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"你是一个专业的知识助手。\"),\n",
        "    HumanMessage(content=\"你知道baichuan2模型吗？\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw-ylWZMQW5z",
        "outputId": "f4ff154a-c8af-457c-a2db-fa310b92ccc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "我不熟悉\"baichuan2\"模型，可能是一个特定领域或领域内的术语。如果您能提供更多背景信息或上下文，我可以尝试为您提供更准确的信息。\n"
          ]
        }
      ],
      "source": [
        "res = chat(messages)\n",
        "print(res.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXir07e9yKNW"
      },
      "source": [
        "chatgpt AI无法满足我们在某些特定领域的专业需求，我们可以通过知识注入的方式，利用prompt来解决这种问题："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6MTJRA2nQW_E"
      },
      "outputs": [],
      "source": [
        "\n",
        "baichuan2_information = [\n",
        "    \"Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\",\n",
        "    \"Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\",\n",
        "    \"Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(baichuan2_information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHKTgq08NhRp",
        "outputId": "d6333937-c854-4b1e-8a12-2bc1d1a33301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\n",
            "Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\n",
            "Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\n"
          ]
        }
      ],
      "source": [
        "print(source_knowledge)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pdgyyDx2yx8M"
      },
      "outputs": [],
      "source": [
        "query = \"你知道baichuan2模型吗？\"\n",
        "\n",
        "prompt_template = f\"\"\"基于以下内容回答问题：\n",
        "\n",
        "内容:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZwSPm_3qzWB7"
      },
      "outputs": [],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=prompt_template\n",
        ")\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P861bTreziWz",
        "outputId": "62aaf1ba-aebe-4576-c5cc-7203f677d205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "是的，我知道Baichuan 2模型。它是一个大规模多语言语言模型，专注于训练在多种语言中表现优异的模型。Baichuan 2使用了庞大的训练数据集，包含了2.6万亿个标记，从头开始训练。相比以往的模型，Baichuan 2提供了更丰富的数据资源，能够更好地支持多语言的开发和应用。Baichuan 2不仅在通用任务上表现出色，还在特定领域的任务中展现了卓越性能，为特定领域的应用提供了强有力的支持。\n"
          ]
        }
      ],
      "source": [
        "print(res.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Npmwyy808i6"
      },
      "source": [
        "当我们注入一些专业的知识后，模型就能够很好的回答相关问题。\n",
        "如果每一个问题都去用相关的外部知识进行增强拼接的话，那么回答的准确性就大大增加？？？？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXfTkYm01oWp"
      },
      "source": [
        "### 创建一个RAG对话模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8TTksfT2K3r"
      },
      "source": [
        "#### 1. 加载数据 （以baichuan2论文为例）\n",
        "\n",
        "   https://arxiv.org/pdf/2309.10305v2.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIVOnz1TxkZ4",
        "outputId": "aeeee9d2-98e0-42b1-b8f5-8ef430eb7f98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.4\n"
          ]
        }
      ],
      "source": [
        "! pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VD-UF8z06txb"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2309.10305.pdf\")\n",
        "\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KsCy_vTs68I-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\\nTao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng\\nXiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang\\nYiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu\\nBaichuan Inc.\\nAbstract\\nLarge language models (LLMs) have\\ndemonstrated remarkable performance on\\na variety of natural language tasks based\\non just a few examples of natural language\\ninstructions, reducing the need for extensive\\nfeature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,\\nBaichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 2022). This increase in scale has\\nled to significant improvements in the capabilities\\nof language models, enabling more human-like\\nfluency and the ability to perform a diverse range\\nof natural language tasks. With the introduction of\\nAuthors are listed alphabetically, correspondent:\\ndaniel@baichuan-inc.com.ChatGPT (OpenAI, 2022) from OpenAI, the power\\nof these models to generate human-like text has\\ncaptured widespread public attention. ChatGPT\\ndemonstrates strong language proficiency across\\na variety of domains, from conversing casually to\\nexplaining complex concepts. This breakthrough\\nhighlights the potential for large language models\\nto automate tasks involving natural language\\ngeneration and comprehension.\\nWhile there have been exciting breakthroughs\\nand applications of LLMs, most leading LLMs like\\nGPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),\\nand Claude (Claude, 2023) remain closed-sourced.\\nDevelopers and researchers have limited access to\\nthe full model parameters, making it difficult for\\nthe community to deeply study or fine-tune these\\nsystems. More openness and transparency around\\nLLMs could accelerate research and responsible\\ndevelopment within this rapidly advancing field.\\nLLaMA (Touvron et al., 2023a), a series of large\\nlanguage models developed by Meta containing up\\nto 65 billion parameters, has significantly benefited\\nthe LLM research community by being fully open-\\nsourced. The open nature of LLaMA, along with\\nother open-source LLMs such as OPT (Zhang\\net al., 2022), Bloom (Scao et al., 2022), MPT\\n(MosaicML, 2023) and Falcon (Penedo et al.,\\n2023), enables researchers to freely access the\\nmodels for examination, experimentation, and\\nfurther development. This transparency and access\\ndistinguishes LLaMA from other proprietary\\nLLMs. By providing full access, the open-source\\nLLMs have accelerated research and advances in\\nthe field, leading to new models like Alpaca (Taori\\net al., 2023), Vicuna (Chiang et al., 2023), and', metadata={'source': '/tmp/tmpw4yahac0/tmp.pdf', 'page': 0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZN6yzwi7J61"
      },
      "source": [
        "#### 2. 知识切片 将文档分割成均匀的块。每个块是一段原始文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wt3G5-ph7gho"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCXqYY4D7gkp",
        "outputId": "d92b8a52-fe1e-4480-db30-1aa2f15b6716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgABYtKp8_Ke"
      },
      "source": [
        "#### 3. 利用embedding模型对每个文本片段进行向量化，并储存到向量数据库中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AmI_-A1-ziZN"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "\n",
        "embed_model = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents=docs, embedding=embed_model , collection_name=\"openai_embed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-fgbDgQC77h"
      },
      "source": [
        "#### 4. 通过向量相似度检索和问题最相关的K个文档。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zuoRfbU_Du3S"
      },
      "outputs": [],
      "source": [
        "query = \"How large is the baichuan2 vocabulary?\"\n",
        "result = vectorstore.similarity_search(query ,k = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dliY5xHaC2NN",
        "outputId": "40191eee-e17e-4283-e63b-f5ff2088bdf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='languages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,', metadata={'page': 1, 'source': '/tmp/tmpw4yahac0/tmp.pdf'}),\n",
              " Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun', metadata={'page': 0, 'source': '/tmp/tmpw4yahac0/tmp.pdf'})]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymUjel7-E-t1"
      },
      "source": [
        "#### 5. 原始`query`与检索得到的文本组合起来输入到语言模型，得到最终的回答"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9wBIBDnIC2P8"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "  # 获取top3的文本片段\n",
        "  results = vectorstore.similarity_search(query, k=3)\n",
        "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "  # 构建prompt\n",
        "  augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "  contexts:\n",
        "  {source_knowledge}\n",
        "\n",
        "  query: {query}\"\"\"\n",
        "  return augmented_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JHTutK09GRSx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using the contexts below, answer the query.\n",
            "\n",
            "  contexts:\n",
            "  languages, such as Chinese.\n",
            "In this technical report, we introduce Baichuan\n",
            "2, a series of large-scale multilingual language\n",
            "models. Baichuan 2 has two separate models,\n",
            "Baichuan 2-7B with 7 billion parameters and\n",
            "Baichuan 2-13B with 13 billion parameters. Both\n",
            "models were trained on 2.6 trillion tokens, which\n",
            "to our knowledge is the largest to date, more than\n",
            "double that of Baichuan 1 (Baichuan, 2023b,a).\n",
            "With such a massive amount of training data,\n",
            "Baichuan 2: Open Large-scale Language Models\n",
            "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\n",
            "Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\n",
            "Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\n",
            "Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\n",
            "Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\n",
            "adequate training of each word embedding. We\n",
            "have taken both these aspects into account. We\n",
            "have expanded the vocabulary size from 64,000\n",
            "in Baichuan 1 to 125,696, aiming to strike a\n",
            "balance between computational efficiency and\n",
            "model performance.\n",
            "Tokenizer V ocab Size Compression Rate ↓\n",
            "LLaMA 2 32,000 1.037\n",
            "Bloom 250,680 0.501\n",
            "ChatGLM 2 64,794 0.527\n",
            "Baichuan 1 64,000 0.570\n",
            "Baichuan 2 125,696 0.498\n",
            "Table 2: The vocab size and text compression rate of\n",
            "\n",
            "  query: How large is the baichuan2 vocabulary?\n"
          ]
        }
      ],
      "source": [
        "print(augment_prompt(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPNBZlRPGlDB",
        "outputId": "8d453129-f6c5-4553-877f-b692c337efdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "根据上下文中的信息，Baichuan 2的词汇量为125,696。\n"
          ]
        }
      ],
      "source": [
        "# 创建prompt\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(query)\n",
        ")\n",
        "\n",
        "messages.append(prompt)\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpwaPwJteHz3"
      },
      "source": [
        "### 没有OPENAI api key怎么办 创建一个非openai的对话模型  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgKpiexG-AS_"
      },
      "source": [
        "\n",
        "1.   embedding模型  \n",
        "2.   chat模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ExJrgFacesTo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/water/.local/lib/python3.10/site-packages (from sentence-transformers) (4.33.2)\n",
            "Requirement already satisfied: tqdm in /home/water/.local/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /home/water/.local/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /home/water/.local/lib/python3.10/site-packages (from sentence-transformers) (1.26.0)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /home/water/.local/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/water/.local/lib/python3.10/site-packages (from sentence-transformers) (0.17.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/water/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in /home/water/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /home/water/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/water/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: requests in /home/water/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
            "Requirement already satisfied: networkx in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.6.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: sympy in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/water/.local/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n",
            "Requirement already satisfied: setuptools in /home/water/.local/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (68.2.2)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.37.1)\n",
            "Requirement already satisfied: cmake in /home/water/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.5)\n",
            "Requirement already satisfied: lit in /home/water/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/water/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/water/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/water/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /home/water/.local/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torch>=1.6.0\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torch>=1.6.0\n",
            "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torch>=1.6.0\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2020.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/water/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/water/.local/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=85e6b804e509e4bcfd7846db2d051c39ea820f3a4dbe2888ff7691404482c5b5\n",
            "  Stored in directory: /home/water/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: threadpoolctl, scipy, pillow, joblib, scikit-learn, nltk, torchvision, sentence-transformers\n",
            "Successfully installed joblib-1.3.2 nltk-3.8.1 pillow-10.2.0 scikit-learn-1.3.2 scipy-1.11.4 sentence-transformers-2.2.2 threadpoolctl-3.2.0 torchvision-0.15.2\n"
          ]
        }
      ],
      "source": [
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xvD4mIBCHKjG"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "model_name = \"sentence-transformers/sentence-t5-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SzFKi_bQehhX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/water/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading .gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 3.78MB/s]\n",
            "Downloading 1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 534kB/s]\n",
            "Downloading 2_Dense/config.json: 100%|██████████| 116/116 [00:00<00:00, 472kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 3.15M/3.15M [00:01<00:00, 2.12MB/s]\n",
            "Downloading README.md: 100%|██████████| 2.02k/2.02k [00:00<00:00, 6.80MB/s]\n",
            "Downloading config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 4.31MB/s]\n",
            "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 300kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 670M/670M [01:16<00:00, 8.75MB/s] \n",
            "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 243kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 7.51MB/s]\n",
            "Downloading spiece.model: 100%|██████████| 792k/792k [00:01<00:00, 779kB/s]\n",
            "Downloading tokenizer.json: 100%|██████████| 1.39M/1.39M [00:01<00:00, 1.38MB/s]\n",
            "Downloading tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 5.62MB/s]\n",
            "Downloading modules.json: 100%|██████████| 461/461 [00:00<00:00, 1.75MB/s]\n"
          ]
        }
      ],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jJP5K7J5ehlB"
      },
      "outputs": [],
      "source": [
        "vectorstore_hf = Chroma.from_documents(documents=docs, embedding=embedding , collection_name=\"huggingface_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = vectorstore_hf.similarity_search(query ,k = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content='languages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,', metadata={'page': 1, 'source': '/tmp/tmpw4yahac0/tmp.pdf'}), Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun', metadata={'page': 0, 'source': '/tmp/tmpw4yahac0/tmp.pdf'})]\n"
          ]
        }
      ],
      "source": [
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "通过本地部署的模型进行交互"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i8TTksfT2K3r",
        "OZN6yzwi7J61",
        "lgABYtKp8_Ke",
        "ymUjel7-E-t1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e42f4231586464abadc5674077b5b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_881fd88d52904346a58d3ed6b7b25b42",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d32c664e3a94141a49200a3bf815719",
            "value": 5
          }
        },
        "17569993d70142caaca7d550eaf84773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "462fdec12c174c7f893296108744867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d32c664e3a94141a49200a3bf815719": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a2bda030860431a9ff1bc8719f15d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17569993d70142caaca7d550eaf84773",
            "placeholder": "​",
            "style": "IPY_MODEL_c972648b06754d43ada362e512fea865",
            "value": " 5/7 [00:49&lt;00:20, 10.03s/it]"
          }
        },
        "7f7bd9c4ac8441ad9d252869f75b17e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853287ee89244e169ffebbd4a7a0335d",
            "placeholder": "​",
            "style": "IPY_MODEL_462fdec12c174c7f893296108744867b",
            "value": "Loading checkpoint shards:  71%"
          }
        },
        "853287ee89244e169ffebbd4a7a0335d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "881fd88d52904346a58d3ed6b7b25b42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c972648b06754d43ada362e512fea865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0c0568f4d6f4bda9253932577becf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f7bd9c4ac8441ad9d252869f75b17e6",
              "IPY_MODEL_0e42f4231586464abadc5674077b5b85",
              "IPY_MODEL_7a2bda030860431a9ff1bc8719f15d9c"
            ],
            "layout": "IPY_MODEL_d80d0f6e4afb4e709e6a01e3d651b7bf"
          }
        },
        "d80d0f6e4afb4e709e6a01e3d651b7bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
